---
title: "Thorp Midterm Project"
author: "Aidan Thorp"
format: html
editor: visual
---

## Preparing Strawberry Data for Analysis

My Assignment is to clean, organize, and explore the strawberry data set. Then turn in a report that describes how my work has set the stage for further analysis and model building.

## The Data:

I am already given that the data set contains strawberry farming data with details about conventional and organic cultivation. These data include information about chemicals used in strawberry farming, as well as sales, revenue and expense details.

First I will load all the necessary libraries needed to complete this assignment:

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(stringr)
```

Next I extracted the strawb_mar6.csv file and moved it to strawb_data. I also wanted to have a quick look at what is inside strawb_data so I did some early exploring of the data set.

```{r}
# Load the dataset
strawb_data <- read.csv("strawb_mar6.csv")

# View the first few rows
head(strawb_data)

# Check column names and structure
str(strawb_data)

# Summary statistics
summary(strawb_data)
```

## Data Cleaning

Next I knew from the assignment description I would only need to focus on the states California and Florida, so I filtered strawb_data to only include data where the state is one of those two.

```{r}
strawb_filtered <- strawb_data %>%
  filter(State %in% c("CALIFORNIA", "FLORIDA"))

head(strawb_filtered)
```

I also wanted to get rid of any unnecessary columns with nothing in them so after looking at the initial heading of the data, I noticed that these columns seemed to have nothing in them. I wanted to check before blindly removing them so I looked to see all unique values in each of these columns. If my suspicions were correct I would remove the column. If not, I would keep the column.

```{r}
# Define the columns to check
cols_to_check <- c("Week.Ending", "Ag.District", "Ag.District.Code", "County", 
                   "County.ANSI", "Zip.Code", "Region", "watershed_code", "Watershed")

# Loop through the columns and print unique values
for (col in cols_to_check) {
  cat("Unique values in", col, ":\n")
  print(unique(strawb_filtered[[col]]))
  cat("\n----------------------\n")
}
```

My suspicions were correct and all of the columns had nothing in them so I will drop all of these columns as they won't help me with my project.

```{r}
strawb_filtered <- strawb_filtered %>%
  select(-c(Week.Ending, Ag.District, Ag.District.Code, County, 
            County.ANSI, Zip.Code, Region, watershed_code, Watershed))

head(strawb_filtered)
```

I also noticed that Value was being kept as a character, which isn't helpful to me. Instead, I converted it to a numeric type, and if the item in Value wasn't a number, it became NA

```{r}

strawb_filtered <- strawb_filtered %>%
  mutate(Value = suppressWarnings(as.numeric(parse_number(Value))))
```

Next I wanted to split the Strawberry data into two parts. One for Census data and Survey data.

```{r}
# Split into two datasets
strawb_census <- strawb_filtered %>% filter(Program == "CENSUS")
strawb_survey <- strawb_filtered %>% filter(Program == "SURVEY")
```

After the original split I still wanted to clean some of the columns a little more. Specifically I wanted to split the Domain.Category up into 3 separate columns. They would be Chemical, Type, and Chemical Name. This way it would be easier to drill deeper into what is happining.

One other quick thing I noticed is that CV had nothing in it for survey data so I decided to drop it in this data set.

```{r}


# Separate 'Domain' into 'Chemical' and 'Type'
strawb_survey <- strawb_survey %>%
  separate(Domain, into = c("Chemical", "Type"), sep = ", ", extra = "merge", fill = "right")

# Give Chemical Name its own column and drop Domain.Category as it is no longer
# necessary

strawb_survey <- strawb_survey %>%
  mutate(`Chemical Name` = str_extract(Domain.Category, "\\((.*?)\\)")) %>% 
  mutate(`Chemical Name` = str_remove_all(`Chemical Name`, "[()]")) %>%
  mutate(`Chemical Name` = str_remove(`Chemical Name`, " = \\d+$")) %>%
  select(-Domain.Category)

#Drop CV column from strawb_survey
strawb_survey <- select(strawb_survey, -CV....)


```

Now I wanted to split up my data a little more so it was better organized. I wanted 3 smaller data sets for each item in chemical. One would be for Total, the other two would be for Chemical and Fertilizer respectively.

```{r}
# Create filtered data sets
strawb_survey_total <- strawb_survey %>% filter(Chemical == "TOTAL")
strawb_survey_chem  <- strawb_survey %>% filter(Chemical == "CHEMICAL")
strawb_survey_fert  <- strawb_survey %>% filter(Chemical == "FERTILIZER")

head(strawb_survey_chem)
head(strawb_survey_total)
head(strawb_survey_fert)
head(strawb_census)
```

## Visualizations

I was then satisfied that my data was cleaner and better organized I began to think about interesting visualizations that could be insightful. Something that seemed interesting to me was seeing what types of chemicals are used in each state.

I first made a bar graph with the total amount of each type of chemical while also comparing which states use more of each type of chemical

```{r}
ggplot(strawb_survey_chem, aes(x = `Type`, fill = State)) +
  geom_bar(stat = "count", position = "dodge") +
  labs(title = "Total Chemical Usage by State", x = "Chemical Type", y = "Count")
```

There were several interesting things withing this bar chart. Specifically I found it interesting how many insecticides California uses. Another interesting thing I noticed was herbicides are the only chemical type that are used more in Florida.

Next I wanted to get a full list of all the chemicals in the data set and pick 3 to examine further

```{r}
unique(strawb_survey_chem$`Chemical Name`)

```



### Bar chart

I decided to pick Sulfur, Thiram, and Potash as my three chemicals to examine further.

```{r}
# Define the chemicals of interest
chemicals_of_interest <- c("SULFUR", "THIRAM", "POTASH")

# Filter the data for these chemicals
filtered_data <- strawb_survey %>%
  filter(`Chemical Name` %in% chemicals_of_interest)

# Aggregate data to see usage patterns for each state and chemical
usage_summary <- filtered_data %>%
  group_by(State, `Chemical Name`) %>%
  summarise(Total_Usage = sum(Value, na.rm = TRUE))

# Plot the data
ggplot(usage_summary, aes(x = `Chemical Name`, y = Total_Usage, fill = State)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Chemical Usage in Strawberry Farming by State",
       x = "Chemical Name", 
       y = "Total Usage",
       fill = "State")
```

Interestingly these chemicals are used in large volumes in each of the states. Potash seems like it is used more often in Florida and less in California, but the most interesting part of this graph was how much sulfur is used in California strawberries and how it isn't used in Florida strawberries at all.

This could be interesting to look at in a future project to see why these products are used so often/little in each state and examine further how these chemicals help/hurt things about the strawberries such as sales and production of them.

### Bar chart

Next I wanted to see the total value coming out of each state as it would give me a better perspective of who's producing more strawberries.

```{r}
# Aggregate total value by state
state_value_summary <- strawb_census %>%
  group_by(State) %>%
  summarise(TotalValue = sum(Value, na.rm = TRUE))

# Create a bar plot of total value by state
ggplot(state_value_summary, aes(x = reorder(State, TotalValue), y = TotalValue)) +
  geom_bar(stat = "identity", fill = "lightgreen", color = "black") +
  labs(title = "Total Value by State", 
       x = "State", 
       y = "Total Value") +
  theme_minimal() +
  coord_flip()  # Flip coordinates to make it easier to read state names
```

Not surprisingly California produces far more strawberries than Florida. California's total value was over 4 times more than Florida's.

# Histogram

Next I wanted to see what the most common Values were as it would give me a better understanding of the frequency of strawberries are being moved for each state.

```{r}
ggplot(strawb_census, aes(x = Value, fill = State)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +
  labs(title = "Distribution of Value in Strawb Census Data by State", 
       x = "Value", 
       y = "Frequency")
```

It looks like the most common value was on the smaller end and possibly close to zero for both states, but I also noticed how almost all the large outliers are from California. This could mean that California is moving some of its strawberries in large bulk amounts while Florida isn't.

# Scatterplot

Finally, I wanted to see if there was a correlation between Value and the Coefficeint of Variation.

```{r}
# Create a scatter plot of 'Value' vs 'CV....'
ggplot(strawb_census, aes(x = CV...., y = Value)) +
  geom_point(color = "blue", alpha = 0.6) +
  labs(title = "Value vs. Coefficient of Variation (CV....)", 
       x = "Coefficient of Variation (CV....)", 
       y = "Value")
```

It looks like the data stays pretty constant with one type of value which makes sense because in the last visual we saw that the highest frequency was right around zero, but when there is an increase in value there does seem to be some trend on increase that could be looked at further in a future project.
